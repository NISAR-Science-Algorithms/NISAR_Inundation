{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924093ab-d955-40e3-b49d-d42198c94854",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/NISAR_artist_concept.jpg\" width=400 align=\"left\"/><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/NISAR_Mission_Logo.png\" width=400 align=\"left\"/><br><br><br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2beb95c-e132-4b68-a4fd-bf5c63adbc61",
   "metadata": {},
   "source": [
    "# NASA ISRO Synthetic Aperture Radar Mission\n",
    "## Combined Algorithm Theoretical Basis Document and Jupyter Notebook for <br> *Classification of Wetland Inundation Extent*\n",
    "\n",
    "\n",
    "Authors: Bruce Chapman, Paul Siqueira\n",
    "\n",
    "Date: February 15, 2022\n",
    "\n",
    "Last updated: December 2024, Brandi Downs\n",
    "\n",
    "### Summary\n",
    "This notebook describes the ATBD for generating a wetland inundation product from NISAR time series data stacks. First, the images of the multi-temporal sequence must be well radiometrically calibrated relative to each other, to a higher precision than perhaps required through routine standard calibration of the NISAR imagery. This optional calibration step examines distributed targets that are expected to be unchanged or minimally changed in brightness over a set time span of  an image sequence. With NISARâ€™s 240 km swath width, it is reasonably assumed that a statistically large area, A<sub>ni</sub>, will not be inundated (or otherwise changing) during any of the 2n observations surrounding the image to be calibrated and classified. These areas will be identified through use of a priori wetlands mask and partly through image segmentation or other methods over the 2n images. <br>\n",
    "A set of classes will be identified from a multitemporal average of a subset of images including:\n",
    "\n",
    "- Inundated vegetation (presumption: dominated by double bounce scatter in HH channel) \n",
    "- Open water (presumption: low specular scattering in both channels)\n",
    "- Not inundated (presumption: brighter specular scatter, volume scattering)\n",
    "- Not classified (presumption: pixels do not align with the scattering model, or no data)\n",
    "\n",
    "These classes are selected based on calibrated threshold values for the radar backscatter and other metrics. In addition, this same multi-temporal image sequence allows the algorithm to include a more sensitive change detection component for improved robustness. Change detection will allow for refinement within the multitemporal image sequence for change of class during the image sequence that may be more robust than simply classifying the image backscatter and backscatter ratio values.  \n",
    "\n",
    "\n",
    "### Use environment:\n",
    "`ecosystems_atbd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44036c-cc37-4c12-bc71-1c58dc0a3261",
   "metadata": {},
   "source": [
    "## Step 1: Read in GCOV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5cd6b-9093-4ba0-a124-496b65753f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from yaml import safe_load, safe_dump\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from osgeo import gdal, osr\n",
    "import rasterio\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92598a4-0551-4564-b75e-01ea18e19be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames nested within S3 bucket directories\n",
    "calval_site = 'Yucatan_Lake'\n",
    "gcov_url = (\"s3://nisar-st-data-ondemand/ALOS2_processed/Yucatan_Lake_Louisiana/51/630/GCOV/\")\n",
    "s3 = s3fs.S3FileSystem()\n",
    "gcov_dirs = ['s3://' + k['Key'] + '/' for k in s3.listdir(gcov_url)]\n",
    "\n",
    "fstr = '.h5'\n",
    "stats_str = 'STATS'\n",
    "gcov_files = []\n",
    "for k in gcov_dirs:\n",
    "    temp = ['s3://' + f['Key'] for f in s3.listdir(k)]\n",
    "    for j in temp:\n",
    "        if fstr in j and stats_str not in j:\n",
    "            gcov_files.append(j)\n",
    "\n",
    "# sort by date\n",
    "gcov_files = sorted(gcov_files)\n",
    "\n",
    "num_files = len(gcov_files)\n",
    "\n",
    "print(num_files, 'files')\n",
    "print(*gcov_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c4271-cd29-4963-9b4f-96b867653b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all h5 files\n",
    "h5_files = [h5py.File(s3.open(k, \"rb\")) for k in gcov_files]\n",
    "gcov_filenames = [k.filename.strip('<').strip('>').split('/')[-1] for k in h5_files]\n",
    "gcov_dates = [ (str(k.split('_')[11][:4]) + '-' + str(k.split('_')[11][4:6]) + '-' + str(k.split('_')[11][6:8])) for k in gcov_filenames]\n",
    "gcov_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4dafbd-853a-4597-9c82-3732e0fbf5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all groups and datasets in H5 file\n",
    "def print_all_objs(name, obj):\n",
    "    print(obj)\n",
    "\n",
    "#h5_files[0].visititems(print_all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf5a79-49a1-4c0e-b11a-714bb8397a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get xy coordinates, epsg, and backscatter data\n",
    "# This cell may take a few minutes to run if there are many files in h5_files\n",
    "\n",
    "ds_x = [f['science']['LSAR']['GCOV']['grids']['frequencyA']['xCoordinates'][()] for f in h5_files]\n",
    "ds_y = [f['science']['LSAR']['GCOV']['grids']['frequencyA']['yCoordinates'][()] for f in h5_files]\n",
    "ds_epsg = [f['science']['LSAR']['GCOV']['grids']['frequencyA']['projection'][()].item() for f in h5_files]\n",
    "ds_HHHH = [f['science']['LSAR']['GCOV']['grids']['frequencyA']['HHHH'][()] for f in h5_files] \n",
    "ds_HVHV = [f['science']['LSAR']['GCOV']['grids']['frequencyA']['HVHV'][()] for f in h5_files] \n",
    "\n",
    "[f.close() for f in h5_files];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8c4a9-98f9-475f-b26e-69b32c8b3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure x and y arrays are the same for each scene\n",
    "# if they are - keep just 1 copy\n",
    "# if not, print warning\n",
    "\n",
    "x_equal, y_equal, epsg_equal = True, True, True\n",
    "\n",
    "for i,k in enumerate(ds_x):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if ( (k != ds_x[0]).any() ):\n",
    "        x_equal = False\n",
    "        warning_text = 'x array index ' + str(i) + ' unequal'\n",
    "        warnings.warn(warning_text)\n",
    "\n",
    "for i,k in enumerate(ds_y):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if ( (k != ds_y[0]).any() ):\n",
    "        y_equal = False\n",
    "        warning_text = 'x array index ' + str(i) + ' unequal'\n",
    "        warnings.warn(warning_text)\n",
    "\n",
    "for i,k in enumerate(ds_epsg):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if ( k != ds_epsg[0] ):\n",
    "        epsg_equal = False\n",
    "        warning_text = 'epsg array index ' + str(i) + ' unequal'\n",
    "        warnings.warn(warning_text)\n",
    "\n",
    "\n",
    "print('x arrays equal:', x_equal)\n",
    "print('y arrays equal:', y_equal)\n",
    "print('epsg values equal:', epsg_equal)\n",
    "\n",
    "if x_equal:\n",
    "    ds_x = ds_x[0]\n",
    "if y_equal:\n",
    "    ds_y = ds_y[0]\n",
    "if epsg_equal:\n",
    "    ds_epsg = ds_epsg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c47483-b88f-4007-95ba-36331abc71d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the GCOV images\n",
    "\n",
    "fig, axs = plt.subplots(num_files, 2, figsize=(12,num_files*5))\n",
    "cbar_shrink = 0.6\n",
    "\n",
    "for k in range(num_files):\n",
    "\n",
    "    im1 = axs[k][0].imshow(ds_HHHH[k], vmin=0, vmax=0.5, cmap='gray')\n",
    "    title_str = gcov_dates[k] + ', ' + 'HHHH'\n",
    "    axs[k][0].set_title(title_str)\n",
    "    fig.colorbar(im1, ax=axs[k][0], shrink=cbar_shrink)\n",
    "\n",
    "    im2 = axs[k][1].imshow(ds_HVHV[k], vmin=0, vmax=0.1, cmap='gray')\n",
    "    title_str = gcov_dates[k] + ', ' + 'HVHV'\n",
    "    axs[k][1].set_title(title_str)\n",
    "    fig.colorbar(im2, ax=axs[k][1], shrink=cbar_shrink)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc521c-7ca7-4506-ab5a-e9149efc0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio, product, and sum\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    ds_ratio = [ds_HHHH[k]/ds_HVHV[k] for k in range(num_files)]\n",
    "    ds_product = [ds_HHHH[k]*ds_HVHV[k] for k in range(num_files)]\n",
    "    ds_sum = [ds_HHHH[k]+ds_HVHV[k] for k in range(num_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292bcb9-56ee-44c5-96db-de0c4fb2d71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot ratio, product, sum\n",
    "fig, axs = plt.subplots(num_files, 3, figsize=(14,num_files*5))\n",
    "cbar_shrink = 0.6\n",
    "\n",
    "for k in range(num_files):\n",
    "\n",
    "    im1 = axs[k][0].imshow(ds_ratio[k], vmin=1, vmax=12, cmap='gray')\n",
    "    title_str = gcov_dates[k] + ', ' + 'ratio'\n",
    "    axs[k][0].set_title(title_str)\n",
    "    fig.colorbar(im1, ax=axs[k][0], shrink=cbar_shrink)\n",
    "\n",
    "    im2 = axs[k][1].imshow(ds_product[k], vmin=0, vmax=0.05, cmap='gray')\n",
    "    title_str = gcov_dates[k] + ', ' + 'product'\n",
    "    axs[k][1].set_title(title_str)\n",
    "    fig.colorbar(im2, ax=axs[k][1], shrink=cbar_shrink)\n",
    "\n",
    "    im3 = axs[k][2].imshow(ds_sum[k], vmin=0, vmax=0.5, cmap='gray')\n",
    "    title_str = gcov_dates[k] + ', ' + 'sum'\n",
    "    axs[k][2].set_title(title_str)\n",
    "    fig.colorbar(im3, ax=axs[k][2], shrink=cbar_shrink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29643f33-9476-43bd-b123-ebf23dec076b",
   "metadata": {},
   "source": [
    "## Step 2: Read in the classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b990bb-67be-428f-bc3d-da0e2ac2b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in contents of yaml configuration file\n",
    "with s3.open('nisar-st-data-ondemand/wetlands_config/wetland_calibration_parameters.yaml', 'rb') as f:\n",
    "    config_doc = safe_load(f)\n",
    "\n",
    "class_thresh = config_doc['runconfig']['calval_sites'][calval_site]\n",
    "class_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f0004-fe42-4c2e-b18b-149873b6f76f",
   "metadata": {},
   "source": [
    "From the classification thresholds for this calval site, Yucatan Lake, printed above, we see that only `inun_veg_single_class` and `open_water` are used. For `open_water`, the sum is used, not the product. We will save each threshold next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ece7e-c5fc-4b2a-995e-8275ac923768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save thresholds\n",
    "comb_method = 'sum'  # combination method is either product or sum; in this case, it's sum\n",
    "th = {}\n",
    "\n",
    "# inundated vegetation (iv)\n",
    "th['iv_hh_max'] = class_thresh['inun_veg_single_class']['HH_max']\n",
    "th['iv_hh_min'] = class_thresh['inun_veg_single_class']['HH_min']\n",
    "th['iv_ratio_max'] = class_thresh['inun_veg_single_class']['ratio_max']\n",
    "th['iv_ratio_min'] = class_thresh['inun_veg_single_class']['ratio_min']\n",
    "\n",
    "# open water (ow)\n",
    "if comb_method == 'sum':\n",
    "    th['ow_comb_max'] = class_thresh['open_water']['sum_max']\n",
    "    th['ow_comb_min'] = class_thresh['open_water']['sum_min']\n",
    "elif comb_method == 'product':\n",
    "    th['ow_comb_max'] = class_thresh['open_water']['product_max']\n",
    "    th['ow_comb_min'] = class_thresh['open_water']['product_min']    \n",
    "else:\n",
    "    raise Exception(\"Invalid combination method\") \n",
    "\n",
    "th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796b55d-78c0-488e-aafd-56e59a1a50f5",
   "metadata": {},
   "source": [
    "## Step 3: Classify the GCOV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25164c44-98a7-4b64-9280-e5f65bf4654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First classify open water, then classify remaining (non-open water) pixels as inun veg or not inundated\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "nisar_classified_20m = []\n",
    "\n",
    "for k in range(num_files):\n",
    "    temp = np.zeros(ds_HHHH[k].shape, dtype=np.int8)\n",
    "\n",
    "    # set all valid data pixels to 1\n",
    "    if comb_method == 'sum':\n",
    "        ds_comb = ds_sum[k].copy()\n",
    "    else:\n",
    "        ds_comb = ds_product[k].copy()\n",
    "    idx = ds_comb > 0\n",
    "    temp[idx] = 1\n",
    "    numpx = np.sum(temp.ravel())\n",
    "\n",
    "    # set open water pixels to 2\n",
    "    idx = (ds_comb > th['ow_comb_min']) & (ds_comb <= th['ow_comb_max'])\n",
    "    temp[idx] = 2\n",
    "\n",
    "    # set inundate vegetation pixels to 3\n",
    "    idx = (ds_HHHH[k] >= th['iv_hh_min']) & (ds_HHHH[k] <= th['iv_hh_max']) & \\\n",
    "           (ds_ratio[k] >= th['iv_ratio_min']) & (ds_ratio[k] <= th['iv_ratio_max']) & \\\n",
    "           (temp != 2)\n",
    "    temp[idx] = 3\n",
    "\n",
    "    # print('Open water: ' + str(np.round(1000*np.sum(temp.ravel()==2)/numpx)/10) + '%')\n",
    "    # print('Inun veg I: ' + str(np.round(1000*np.sum(temp.ravel()==3)/numpx)/10) + '%')\n",
    "    # print('Not inun: '  + str(np.round(1000*np.sum(temp.ravel()==1)/numpx)/10) + '%')\n",
    "\n",
    "    nisar_classified_20m.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252fbde-1c6e-41ff-9e76-141e82642e37",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot classified images\n",
    "\n",
    "# set up colormaps\n",
    "c_white = (255, 255, 255)\n",
    "c_lightblue = (66, 233, 245)\n",
    "c_darkblue = (21, 27, 115)\n",
    "c_gray = (236, 236, 238)\n",
    "\n",
    "colors = [c_white, c_gray, c_darkblue, c_lightblue]\n",
    "colors2 = []\n",
    "for k in colors:\n",
    "    colors2.append(tuple(np.array(k)/255)) \n",
    "cmap = LinearSegmentedColormap.from_list('cmap_class', colors2, N=4)\n",
    "\n",
    "fig, axs = plt.subplots(num_files, 1, figsize=(6,num_files*5))\n",
    "cbar_shrink = 0.6\n",
    "cbar_ticks = [3/8, 9/8, 15/8, 21/8]\n",
    "cbar_labels = ['no data','not inun','open water','inun veg']\n",
    "# cbar_ticks = [4/10, 12/10, 20/10, 28/10, 36/10]  # for 2 inun veg classes\n",
    "# cbar_label = ['no data','not inun','open water','inun veg I','inun veg II']  # for 2 inun veg classes\n",
    "\n",
    "for k in range(num_files):\n",
    "\n",
    "    im = axs[k].imshow(nisar_classified_20m[k], vmin=0, vmax=3, cmap=cmap, interpolation='nearest')\n",
    "    axs[k].set_title(gcov_dates[k])\n",
    "    cbar = plt.colorbar(im, ax=axs[k], shrink=cbar_shrink)    \n",
    "    cbar.set_ticks(cbar_ticks)\n",
    "    cbar.set_ticklabels(cbar_labels, fontsize=10)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a814449-048f-46c9-aac1-8d8845be164d",
   "metadata": {},
   "source": [
    "## Step 4: Aggregate to 1 Hectare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea94cc5-4972-4c75-bb3c-ba2391652b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output results as geotiff using rasterio\n",
    "\n",
    "class_dir = Path(os.getcwd()) / 'nisar_classifications' / calval_site\n",
    "Path(class_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "classification_20m_filenames = []\n",
    "classification_1ha_filenames = []\n",
    "\n",
    "for k in range(num_files):\n",
    "\n",
    "    # create 20m geotiffs to use as inputs for gdal Warp\n",
    "    filename_20m = str(class_dir / ('nisar_classified_20m_' + datetime.today().strftime('%Y%m%d') + '_gcov_' + \"\".join(gcov_dates[k].split('-')) + \\\n",
    "                      '_' + calval_site + '.tif'))\n",
    "    classification_20m_filenames.append(filename_20m)\n",
    "    meta = {'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': None, \n",
    "            'width': ds_x.shape[0], \n",
    "            'height': ds_y.shape[0], \n",
    "            'count': 1, \n",
    "            'crs': rasterio.CRS.from_epsg(ds_epsg), \n",
    "            'transform': rasterio.Affine(ds_x[1] - ds_x[0], 0.0, ds_x[0] - ((ds_x[1] - ds_x[0])/2), 0.0, ds_y[1] - ds_y[0], ds_y[0] - ((ds_y[1] - ds_y[0])/2)),\n",
    "            'tiled': False, \n",
    "            'interleave': 'band'}\n",
    "    with rasterio.open(filename_20m, 'w', **meta) as dst:\n",
    "        dst.write(nisar_classified_20m[k],indexes=1)    \n",
    "\n",
    "    L3_filename_1ha = str(class_dir / ('nisar_classified_1ha_' + datetime.today().strftime('%Y%m%d') + '_gcov_' + \"\".join(gcov_dates[k].split('-')) + \\\n",
    "                      '_' + calval_site + '.tif'))\n",
    "    classification_1ha_filenames.append(L3_filename_1ha)\n",
    "    gdal.Warp(L3_filename_1ha, filename_20m, xRes=100, yRes=-100, resampleAlg=gdal.GRA_Mode, format=\"COG\")\n",
    "\n",
    "    # optional: remove 20m files\n",
    "    os.remove(filename_20m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f51bc-81c1-4160-835e-b62ec24e7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the 1 ha data\n",
    "\n",
    "nisar_classified_1ha = []\n",
    "\n",
    "ds = rxr.open_rasterio(classification_1ha_filenames[0])\n",
    "ds_1ha_x = ds.x\n",
    "ds_1ha_y = ds.y\n",
    "ds_1ha_epsg = ds.rio.crs.to_authority()[1]\n",
    "\n",
    "for k in classification_1ha_filenames:\n",
    "    ds = rxr.open_rasterio(k)\n",
    "    nisar_classified_1ha.append(ds.to_numpy().squeeze().astype(np.int8))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066f65f-bdfa-4fda-b6bd-96a2beea2a02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot 1-ha classified images\n",
    "\n",
    "# set up colormaps\n",
    "c_white = (255, 255, 255)\n",
    "c_lightblue = (66, 233, 245)\n",
    "c_darkblue = (21, 27, 115)\n",
    "c_gray = (236, 236, 238)\n",
    "\n",
    "colors = [c_white, c_gray, c_darkblue, c_lightblue]\n",
    "colors2 = []\n",
    "for k in colors:\n",
    "    colors2.append(tuple(np.array(k)/255)) \n",
    "cmap = LinearSegmentedColormap.from_list('cmap_class', colors2, N=4)\n",
    "\n",
    "fig, axs = plt.subplots(num_files, 1, figsize=(6,num_files*5))\n",
    "cbar_shrink = 0.6\n",
    "cbar_ticks = [3/8, 9/8, 15/8, 21/8]\n",
    "cbar_labels = ['no data','not inun','open water','inun veg']\n",
    "# cbar_ticks = [4/10, 12/10, 20/10, 28/10, 36/10]  # for 2 inun veg classes\n",
    "# cbar_label = ['no data','not inun','open water','inun veg I','inun veg II']  # for 2 inun veg classes\n",
    "\n",
    "for k in range(num_files):\n",
    "\n",
    "    im = axs[k].imshow(nisar_classified_1ha[k], vmin=0, vmax=3, cmap=cmap, interpolation='nearest')\n",
    "    axs[k].set_title(gcov_dates[k])\n",
    "    cbar = plt.colorbar(im, ax=axs[k], shrink=cbar_shrink)    \n",
    "    cbar.set_ticks(cbar_ticks)\n",
    "    cbar.set_ticklabels(cbar_labels, fontsize=10)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecosystems_atbd",
   "language": "python",
   "name": "ecosystems_atbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
